import torch
import librosa
import numpy as np
import soundfile as sf
import os
from src.model import UNet

# ×”×’×“×¨×•×ª ×—×•××¨×”
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if not torch.cuda.is_available():
    try:
        import torch_directml
        device = torch_directml.device()
    except:
        pass

MODEL_PATH = 'models/generalist_vocals.pth'
INPUT_SONG = 'AUDIO/Manchild.mp3' 
OUTPUT_PATH = 'AUDIO/separated_vocals.wav'

def predict():
    # 1. ×˜×¢×™× ×ª ×”××•×“×œ
    model = UNet(out_channels=1).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print(f"ğŸš€ Model loaded on {device}")

    # 2. ×˜×¢×™× ×ª 5 ×©× ×™×•×ª ×‘×“×™×•×§
    sr = 22050
    duration = 5
    y, _ = librosa.load(INPUT_SONG, sr=sr, duration=duration) 
    
    # ×”×’×“×¨×ª ×¤×¨××˜×¨×™× ×§×‘×•×¢×™× ×œ×©×—×–×•×¨ ××™×›×•×ª×™
    n_fft = 2048
    hop_length = 512 # ×”×¤×¨××˜×¨ ×©×§×•×‘×¢ ××ª ×”"××•×¨×š" ×‘×–××Ÿ
    
    # ×”×¤×™×›×” ×œ×¡×¤×§×˜×¨×•×’×¨××”
    S_full = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=n_fft, hop_length=hop_length)
    S_db = librosa.power_to_db(S_full, ref=np.max)
    
    # × ×™×¨××•×œ (0-1)
    S_norm = (S_db + 80) / 80
    
    # ×”×›× ×” ×œ××•×“×œ
    input_tensor = torch.tensor(S_norm).unsqueeze(0).unsqueeze(0).float().to(device)

    # 3. ×”×¨×¦×” ×“×¨×š ×”××•×“×œ
    print("ğŸµ Separating vocals...")
    with torch.no_grad():
        mask = model(input_tensor)
    
    # 4. ×©×—×–×•×¨ ×”××•×“×™×•
    mask = mask.squeeze().cpu().numpy()
    if mask.shape != S_full.shape:
        import scipy.ndimage
        mask = scipy.ndimage.zoom(mask, (S_full.shape[0]/mask.shape[0], S_full.shape[1]/mask.shape[1]))

    # ×‘××§×•× ×œ×©×—×–×¨ ××”-dB, ×× ×—× ×• ××›×¤×™×œ×™× ××ª ×”×¡×¤×§×˜×¨×•×’×¨××” ×”××§×•×¨×™×ª ×‘××¡×™×›×”
    # ×–×” ×©×•××¨ ×¢×œ ×”×“×™× ××™×§×” ×”××§×•×¨×™×ª ×©×œ ×”×©×™×¨
    S_vocals = S_full * mask 
    
    print("ğŸ”Š Reconstructing audio with original phase...")
    y_out = librosa.feature.inverse.mel_to_audio(S_vocals, sr=sr, n_fft=n_fft, hop_length=hop_length)
    
    # 5. ×©××™×¨×”
    sf.write(OUTPUT_PATH, y_out, sr)
    print(f"âœ… Success! Vocals saved (5 seconds) to: {OUTPUT_PATH}")

if __name__ == "__main__":
    if not os.path.exists('AUDIO'):
        os.makedirs('AUDIO')
    predict()